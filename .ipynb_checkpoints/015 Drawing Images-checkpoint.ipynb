{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Drowing images and shapes using openCV</h4>\n",
    "Let's saer off making ablack square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#create a black image\n",
    "image = np.zeros((512,512, 3), np.uint8)\n",
    "\n",
    "#can we make this in black and wiht?\n",
    "image_bw = np.zeros((512,512), np.uint8)\n",
    "\n",
    "cv2.imshow(\"Black Rectangle Color\", image)\n",
    "cv2.imshow(\"Black Rectangle B&W\", image_bw)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's now draw line over our black square</h4>\n",
    "<p>cv2.line(image, starting cordinates, ending cordinates, color, thikness)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#create a black image\n",
    "image = np.zeros((512,512, 3), np.uint8)\n",
    "cv2.line(image, (0,0), (512,512), (255, 127, 0), 5)\n",
    "cv2.imshow(\"Bllue Line\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's now draw a rectangle</h4>\n",
    "<p>cv2.rectangle(image, starting vertex, opposite vertex, color, thikness)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#create a black image\n",
    "image = np.zeros((512,512, 3), np.uint8)\n",
    "#Drow Line \n",
    "cv2.line(image, (100,100), (300,250), (127, 50, 127), 5)\n",
    "#Drow Rectangle\n",
    "cv2.rectangle(image, (100,100), (300,250), (255, 255, 255), 3)\n",
    "cv2.imshow(\"Rectangle and line\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>How about circle?</h4>\n",
    "<p>cv2.circle(image, center, radius, color, fill)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#create a black image\n",
    "image = np.zeros((512,512, 3), np.uint8)\n",
    "#Drow Line \n",
    "cv2.circle(image, (200,200), 200,  (127, 50, 127), -1)\n",
    "\n",
    "cv2.imshow(\"Circle\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>And Polygon?</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#create a black image\n",
    "image = np.zeros((512,512, 3), np.uint8)\n",
    "\n",
    "#Drow circle, Let's define four points\n",
    "pts = np.array([[10,50], [400, 50], [90,200], [50, 500]], np.int32)\n",
    "\n",
    "# Let's now reshape our Points in form required by polylines\n",
    "pts = pts.reshape((-1,1,2))\n",
    "\n",
    "#Drow Rectangle\n",
    "cv2.polylines(image, [pts], True, (0,0,255), 3)\n",
    "cv2.imshow(\"Polygon\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let’s even add text with cv2.putText</h4>\n",
    "<p>cv2.putText(image, 'Text to Display', bottom left starting point, Font, Font Size, Color, Thickness)</p>\n",
    "<ul>\n",
    "    <li>FONT_HERSHEY_SIMPLEX, FONT_HERSHEY_PLAIN</li>\n",
    "    <li>FONT_HERSHEY_DUPLEX, FONT_HERSHEY_COMPLEX</li>\n",
    "    <li>FONT HERSHEY TRIPLEX, FONT HERSHEY COMPLEX SMAL</li>\n",
    "    <li>FONT_HERSHEY_SCRIPT_SIMPLEX</li>\n",
    "    <li>FONT_HERSHEY_SCRIPT_COMPLEX</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#create a black image\n",
    "image = np.zeros((512,512, 3), np.uint8)\n",
    "\n",
    "cv2.putText(image, 'Hello World', (75,290), cv2.FONT_HERSHEY_COMPLEX, 2,  (127, 50, 127), 3)\n",
    "\n",
    "cv2.imshow(\"Circle\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>017 Translations</h2>\n",
    "<p>THis an affine transform that simply shifts the  position of an image.</p>\n",
    "<p>We use cv2.warpAffine to implement these transformations</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/rose.jpg')\n",
    "height, width = image.shape[0:2]\n",
    "quarter_height, quarter_width = height/4, width/4\n",
    "#     | 1 0 Tx |\n",
    "# T = | 0 1 Ty |\n",
    "\n",
    "# T is our translation matrix\n",
    "T = np.float32([[1, 0, quarter_width], [0, 1, quarter_height]])\n",
    "\n",
    "#we use wrapAffine to transform the image using the matrix, T\n",
    "img_translation = cv2.warpAffine(image, T, (width, height))\n",
    "\n",
    "cv2.imshow(\"Translation\", img_translation)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>018 Rotations</h2>\n",
    "<p>cv2.getRotationMatrix2D(rotation_center_x, rotation_center_y, angle of rotation, scale)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/rose.jpg')\n",
    "height, width = image.shape[0:2]\n",
    "\n",
    "#divided by two to rotate image around its centre\n",
    "rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), 45, .5)\n",
    "\n",
    "rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "\n",
    "cv2.imshow(\"Translation\", rotated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Notice all the black space surrounding the imag.</h4>\n",
    "<p>We could now crop the image as we can calculate it's new size (we haven't learned cropping yet!).</p>\n",
    "<p>But here's another method for simple rotations that uses the cv2.transpose functioin</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/rose.jpg')\n",
    "\n",
    "rotated_image = cv2.transpose(image)\n",
    "\n",
    "cv2.imshow(\"transpose\", rotated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>019 Scaling Re-sizing and Interpolations</h2>\n",
    "<p>Re-sizing is very using the cv2.resize function, it's arguments are :</p>\n",
    "<p>cv2.resize(image, dsize(outPut image size), x scale, y scale, interpolation)</p>\n",
    "<h2>What is interpolation?</h2>\n",
    "\n",
    "<h5>Interpolation is a method of constructing new data points within the range of a discrete set of known data points</h5>\n",
    "\n",
    "<li>cv2.INTER_AREA - Good for shrinking or down sampling</li>\n",
    "<li>cv2.INTER_NEAREST - Fastest</li>\n",
    "<li>cv2.INTER_LINEAR - Good for zooming or up sampling (default)</li>\n",
    "<li>cv2.INTER_CUBIC - Better</li>\n",
    "<li>cv2.INTER_LANCZOS4 - Best</li>\n",
    "<p>Good comparison of interpolation methods:</p>\n",
    "<a href=\"http://tanbakuchi.com/posts/comparison-of-openv-interpolation-algorithms/\n",
    "\">http://tanbakuchi.com/posts/comparison-of-openv-interpolation-algorithms/</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/rose.jpg')\n",
    "\n",
    "#let's make our image 3/4 of it's original size\n",
    "image_scaled = cv2.resize(image, None, fx=0.75, fy=0.75)\n",
    "cv2.imshow(\"Scaling leaner interpolation\", image_scaled)\n",
    "cv2.waitKey()\n",
    "\n",
    "#let's double the size of our image\n",
    "image_scaled = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\n",
    "cv2.imshow(\"Scaling INTER_CUBIC interpolation\", image_scaled)\n",
    "cv2.waitKey()\n",
    "\n",
    "#let's SKEW the size of our image\n",
    "image_scaled = cv2.resize(image, (900, 900), interpolation = cv2.INTER_AREA)\n",
    "cv2.imshow(\"Scaling INTER_CUBIC SKEW\", image_scaled)\n",
    "cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>020 Image Pyramids</h2>\n",
    "<p>Useful when scaling images in object detection.</p>\n",
    "<p>Pyramiding image refers to either upscaling (enlarging) and downscaling (shrinking images). </p>\n",
    "<p>It's simply a different way of re-sizing that allows us to easily and quickly scale images. Scaling down reduces the height and width of the new image by half </p>\n",
    "<p>This comes in useful when making object detectors that scales images each time it looks for an object. </p>\n",
    "<img src=\"images/20.jpg\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/rose.jpg')\n",
    "smaller = cv2.pyrDown(image)\n",
    "larger = cv2.pyrUp(smaller)\n",
    "\n",
    "cv2.imshow('original', image)\n",
    "cv2.imshow('smaller', smaller)\n",
    "cv2.imshow('larger', larger)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>021 Cropping</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/20.jpg')\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "#Ler's get the starting pixel cootdiantes (top left of cropping rectangle)\n",
    "start_row, start_col = int (height * .25), int(width * .25)\n",
    "\n",
    "#Let's get the ending pixle coordinates (bottom right)\n",
    "end_row, end_col = int (height * 0.75), int (width * 0.75)\n",
    "\n",
    "#simply use indexing to crop out the rectangl we desire\n",
    "cropped = image[start_row:end_row, start_col:end_col]\n",
    "\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Cropped Image', cropped)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>022 Arithmetic Operations Brightening and Darkening Images</h2>\n",
    "<p>These are simple operations that allow us to directly add or subract to the color intensity. <p/>\n",
    "<p>Calculates the per-element operation of two arrays. The overall effect is increasing or decreasing brightness. <p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/t.png')\n",
    "cv2.imshow(\"Original\", image)\n",
    "\n",
    "# Create a matrix of ones, then muLtipLy it by a scoter of 100\n",
    "# This gives a matrix with some dimesions of our image with aLL voLues being 100 \n",
    "M = np.ones(image.shape, dtype = \"uint8\") * 100\n",
    "\n",
    "# We use this to add this matrix M, to our image\n",
    "# Notice the increase in brightness\n",
    "added = cv2.add(image, M)\n",
    "cv2.imshow(\"Added\", added)\n",
    "\n",
    "# Likewise we can also subtract\n",
    "# Notice the decrease in brightness \n",
    "subtracted = cv2.subtract(image, M) \n",
    "cv2.imshow(\"Subtracted\", subtracted)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bitwise Operations and Masking </h2>\n",
    "    <p>To demonstrate these operations let's create some simple images</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/t.png')\n",
    "\n",
    "# If you're wondering why only two dimensions, well this is a grayscote image,\n",
    "# if we doing o colored image, we'd use\n",
    "# rectangle = np.zeros((300, 300, 3),np.uint8)\n",
    "\n",
    "# Making a sciore\n",
    "square = np.zeros((300, 300), np.uint8) \n",
    "cv2.rectangle(square, (50, 50), (250, 250), 148, 3)\n",
    "#cv2.imshow(\"Square\", square) \n",
    "cv2.waitKey(0)\n",
    "           \n",
    "# Making a eLLipse \n",
    "ellipse = np.zeros((300, 300), np.uint8)\n",
    "cv2.ellipse(ellipse, (150, 150), (50, 150), 0, 20, 90, 120, -1)\n",
    "cv2.imshow(\"Ellipse\", ellipse) \n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>024 Blurring </h2>\n",
    "    <h3>Copvolutions & Blurring</h3>\n",
    "    <p>A <strong>Convolution</strong> is a mathematical operation performed on two functions producing a third function which is typically a modified version of one of the original functions.</p>\n",
    "    <h4>Output Image = Image × FunctionKernel Size</h4>\n",
    "    <p>In Computer Vision we use <strong>kernel's</strong> to specify the size over which we run our manipulating function over our image.</p>\n",
    "    <img src=\"images/kerenal.jpg\" alt=\"Kerenal\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('images/t.png')\n",
    "cv2.imshow('Original Image', image)\n",
    "\n",
    "#creating out 3 × 3 Kernel\n",
    "kernel_3x3 = np.ones((3,3), np.float32)/9\n",
    "\n",
    "#We use the cv2.filter2D to convolve the kernel with an image\n",
    "blurred = cv2.filter2D(image, -1, kernel_3x3)\n",
    "cv2.imshow('3x3blurred', blurred)\n",
    "\n",
    "#creating our 7 x 7 kernel\n",
    "kernel_7x7 = np.ones((7,7), np.float32)/ 49\n",
    "\n",
    "blurred2 = cv2.filter2D(image, -1, kernel_7x7)\n",
    "cv2.imshow('7x7 kernel Blurring', blurred2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Other commonly used blurring methods in Opencv</h3>\n",
    "<img src=\"images/blur.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('images/t.png')\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "#Averaging done by convoLving the image with a normalized box filter.\n",
    "#This takes the pixeLs under the box and repLaces the centraL eLement\n",
    "#Box size needs to add and positive\n",
    "blur = cv2.blur(image, (3,3))\n",
    "cv2.imshow('Averaging', blur)\n",
    "           \n",
    "#Instead of box fiLter, gaussian kerneL\n",
    "Gaussian = cv2.GaussianBlur(image, (7,7), 0)\n",
    "cv2.imshow('Gaussian Blurring', Gaussian)\n",
    "\n",
    "#Takes median of aLL the pixels under kerneL area and centraL\n",
    "#eLement is repLaced with this median vaLue\n",
    "median = cv2.medianBlur(image, 5)\n",
    "cv2.imshow('Median Blurring', median)\n",
    "\n",
    "# BiLateraL is very effective in noise removaL whiLe keeping edges sharp\n",
    "bilateral = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "cv2.imshow('Bilateral Blurring', bilateral)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Image De-noising - Non-Local Means Denoising</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('images/t.png')\n",
    "\n",
    "# parameters, after None are - the filter strength 'h' (5-10 is a good range)\n",
    "# Next is the hGorColorComponents, set as same value as h again \n",
    "\n",
    "dst = cv2.fastNlMeansDenoisingColored(image, None, 6, 6, 7, 21)\n",
    "\n",
    "cv2.imshow('Fast Means Denoising', dst)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>There are 4 variations of None-Local Means Denoising:</h2>\n",
    "    <li>cv2.fastNlMeansDenoising() - works with a single grayscale images</li>\n",
    "    <li>cv2.fastNlMeansDenoisingColored() - works with a color image.</li>\n",
    "    <li>cv2.fastNlMeansDenoisingMulti() - works with image sequence captured in short period of time (grayscale images)</li>\n",
    "    <li>cv2.fastNlMeansDenoisingColoredMulti()- same as above. but for color images.</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>025 Sharpening</h2>\n",
    "<p>By altering our kernels we can implement shapenig, which has the effects of in strengthening or emphasizing in an image.</p>\n",
    "<img src=\"images/25-sharping.png\" style=\"display: block; margin: 10px auto;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('images/t.png')\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "# Create our shapening kernel, we don't normalize since the values in the matrix sum to 1\n",
    "kernel_sharpening = np.array([[-1, -1, -1],\n",
    "                            [-1, 9, -1],\n",
    "                            [-1, -1, -1]])\n",
    "\n",
    "#Applying different kernels to the input image\n",
    "sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n",
    "\n",
    "cv2.imshow('Image Sharpening', sharpened)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>026 Thresholding Binarization</h1>\n",
    "<p>In threesholding, We convert a grey scale image to it's binary form</p>\n",
    "<img src=\"images/26.png\" style=\"display: block; margin: 10px auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('images/t.png', 0)\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "#Values below 127 goes to 0 (blak, everything above goes to 255 (white))\n",
    "ret, thresh1 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('1 Threeshold Binary', thresh1)\n",
    "\n",
    "#Values below 127 goes to 255 and values above 127 go to 0 (reverse of above)\n",
    "ret, thresh2 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "cv2.imshow('2 Threeshold Binary Inverse', thresh2)\n",
    "\n",
    "#Values below 127 are truncated (held) at 127 (the 255 argument is unused)\n",
    "ret, thresh3 = cv2.threshold(image, 127, 255, cv2.THRESH_TRUNC)\n",
    "cv2.imshow('3 Threeshold Binary Trunc', thresh3)\n",
    "\n",
    "#Values below 127 go to 0, above 127 are unchanged\n",
    "ret, thresh4 = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO)\n",
    "cv2.imshow('4 Threeshold Binary TOZERO', thresh4)\n",
    "\n",
    "#Reverse of the above, 127 is unused, above 127 goes to 0\n",
    "ret, thresh5 = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO_INV)\n",
    "cv2.imshow('5 Threeshold Binary TOZERO_INV', thresh5)\n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Is there a better way off thresholding? </h4>\n",
    "    <p>The biggest downfall of those simple threshold methods is that we need to provide the threshold value (i.e. the 127 value we used previously). \n",
    "        What if there was a smarter way of doing this? \n",
    "        There is with. Adaptive thresholding. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('images/260.png', 0)\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "#Values beLow 127 goes to 0 (bLack, eve); thing above goes to 255 (white) \n",
    "ret,threshn= cv2.threshold(image, 127, 255, cv2.THRESH_BINARY) \n",
    "cv2.imshow('Threshold Binary thresh1', threshn)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#It's good practice to bLur images as it removes noise \n",
    "image = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "\n",
    "#Using odoptiveThreshoLd\n",
    "thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 5)\n",
    "cv2.imshow('Adaptive Mean thresholding', thresh)\n",
    "\n",
    "_, th2 = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) \n",
    "cv2.imshow(\"Otsu's Thresholding\", thresh)\n",
    "\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv2.GaussianBlur(image, (5,5), 0)\n",
    "_, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) \n",
    "cv2.imshow(\"Guassian Otsu's Thresholding\", thresh)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/261.png\" style=\"display: block; margin: 10px auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>027 Dilation Erosion Opening and Closing</h1>\n",
    "<img src=\"images/27.png\" style=\"display: block; margin: 10px auto;\">\n",
    "<img src=\"images/271.png\" style=\"display: block; margin: 10px auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('images/ed.png', 0)\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "#Let's define our kernel size\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "#Now we erode\n",
    "erosion = cv2.erode(image, kernel, iterations = 1)\n",
    "cv2.imshow('Erosion', erosion)\n",
    "\n",
    "dilation = cv2.dilate(image, kernel, iterations = 1)\n",
    "cv2.imshow('Dilation', dilation)\n",
    "\n",
    "# Opening - Good for removing noise\n",
    "opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "cv2.imshow('Opening', opening)\n",
    "\n",
    "#Closing - Good for removing noise\n",
    "closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "cv2.imshow('closing', closing)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>028 Edge Detection and Image Gradients</h1>\n",
    "    <img src=\"images/28.png\" style=\"display: block; margin: 10px auto; max-width:97%;\">\n",
    "    <img src=\"images/281.png\" style=\"display: block; margin: 10px auto; max-width:97%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('images/282.png', 0)\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "height ,width = image.shape\n",
    "\n",
    "#Extract Sobel Edges\n",
    "sobel_x = cv2.Sobel(image, cv2.CV_64F, 0,1, ksize=5)\n",
    "sobel_y = cv2.Sobel(image, cv2.CV_64F, 1,0, ksize=5)\n",
    "\n",
    "cv2.imshow('sobel_x', sobel_x)\n",
    "cv2.imshow('sobel_y', sobel_y)\n",
    "\n",
    "sobel_OR = cv2.bitwise_or(sobel_x, sobel_y)\n",
    "cv2.imshow('sobel_OR', sobel_OR)\n",
    "\n",
    "laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "cv2.imshow('laplacian', laplacian)\n",
    "\n",
    "## Then, we need to provide two values: threshold1 and threshold2, Any gradient value larger than threshold2\n",
    "# is considered to be an edge. Any value below threshold1 is considered not to be an edge.\n",
    "# Values in between threshold1 and threshold2 are eithe classified as edges or non-edges bsed on how their\n",
    "#where as any values above 120 are considered edges.\n",
    "\n",
    "#Canny Edge Detection uses gradient values as thesholds\n",
    "#The first threshold gradient\n",
    "canny = cv2.Canny(image, 20, 170)\n",
    "cv2.imshow('Canny', canny)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>029 Perspective and Affine Transforms</h1>\n",
    "    <img src=\"images/29.png\" style=\"display: block; margin: 10px auto; max-width:97%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Getting Perpsective  Transform</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('images/291.png', 0)\n",
    "cv2.imshow('Original', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Cordinates of the 4 corners of the original image\n",
    "points_A = np.float32([[249,0], [530,150], [65,458],[407,583] ])\n",
    "\n",
    "# Cordinates of the 4 corners of the desired output\n",
    "# We use a ratio of an A4 Paper 1 : 1.41\n",
    "points_B = np.float32([[0,0], [320,0], [0,455], [320,455]])\n",
    "                       \n",
    "# Use the two sets of four points to compute\n",
    "# the Perspective Transformation matrix, M\n",
    "M = cv2.getPerspectiveTransform(points_A, points_B)\n",
    "warped = cv2.warpPerspective(image, M, (320, 455))\n",
    "\n",
    "cv2.imshow('wrapPerspective', warped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import  matplotlib.pyplot as plt\n",
    "image = cv2.imread('images/291.png', 0)\n",
    "rows,cols = image.shape\n",
    "cv2.imshow('Original', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Cordinates of the 4 corners of the original image\n",
    "points_A = np.float32([[249,0], [530,150], [65,458]])\n",
    "\n",
    "# Cordinates of the 4 corners of the desired output\n",
    "# We use a ratio of an A4 Paper 1 : 1.41\n",
    "points_B = np.float32([[0,0], [320,0], [0,455]])\n",
    "                       \n",
    "# Use the two sets of four points to compute\n",
    "# the Perspective Transformation matrix, M\n",
    "M = cv2.getAffineTransform(points_A, points_B)\n",
    "warped = cv2.warpAffine(image, M, (cols, rows))\n",
    "\n",
    "cv2.imshow('wrapPerspective', warped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import  matplotlib.pyplot as plt\n",
    "image = cv2.imread('images/292.png', 0)\n",
    "rows,cols = image.shape\n",
    "cv2.imshow('Original', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Cordinates of the 4 corners of the original image\n",
    "points_A = np.float32([[249,0], [530,150], [65,458]])\n",
    "\n",
    "# Cordinates of the 4 corners of the desired output\n",
    "# We use a ratio of an A4 Paper 1 : 1.41\n",
    "points_B = np.float32([[0,0], [320,0], [0,455]])\n",
    "                       \n",
    "# Use the two sets of four points to compute\n",
    "# the Perspective Transformation matrix, M\n",
    "M = cv2.getAffineTransform(points_A, points_B)\n",
    "warped = cv2.warpAffine(image, M, (cols, rows))\n",
    "\n",
    "cv2.imshow('wrapPerspective', warped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Video Capture Exampel</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(cap.isOpened()):  # check !\n",
    "    # capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret: # check ! (some webcam's need a \"warmup\")\n",
    "        # our operation on frame come here\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame', gray)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# When everything is done release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>030 Mini Project  1 - Live Sketch App</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def sketch(image):\n",
    "    #Convert image to grayscale\n",
    "    #cv2.imshow('sketch(image)', image)\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Clean up image using Guassian Blur\n",
    "    img_gray_blur = cv2.GaussianBlur(img_gray, (5,5), 0)\n",
    "    \n",
    "    #Extract edges\n",
    "    canny_edges = cv2.Canny(img_gray_blur, 10, 70)\n",
    "    \n",
    "    # Do an invert binarize the image\n",
    "    ret, mask = cv2.threshold(canny_edges, 70, 255, cv2.THRESH_BINARY_INV )\n",
    "    return mask\n",
    "\n",
    "# Intialize webcam, cap is the object provided by VideoCapture\n",
    "# It contains a boolean indicating if it was successful (ret)\n",
    "# It also contains the images collected from the webcam (frame)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Our Live Sketcher', sketch(frame))\n",
    "    if cv2.waitKey(1) == 13: #13 is the enter key\n",
    "        break\n",
    "\n",
    "#Release camera and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>031 Segmentation and Contours</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/312.png\" style=\"display: block; margin: 10px auto; max-width:97%;\">\n",
    "<img src=\"images/31.png\" style=\"display: block; margin: 10px auto; max-width:97%;\">\n",
    "<img src=\"images/311.png\" style=\"display: block; margin: 10px auto; max-width:97%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Contours</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numper of Contours found = 3\n",
      "[array([[[271, 119]],\n",
      "\n",
      "       [[271, 228]],\n",
      "\n",
      "       [[366, 228]],\n",
      "\n",
      "       [[366, 119]]]), array([[[386,  48]],\n",
      "\n",
      "       [[385,  49]],\n",
      "\n",
      "       [[385, 233]],\n",
      "\n",
      "       [[386, 234]],\n",
      "\n",
      "       [[386, 235]],\n",
      "\n",
      "       [[584, 235]],\n",
      "\n",
      "       [[584,  48]]]), array([[[238,  14]],\n",
      "\n",
      "       [[237,  15]],\n",
      "\n",
      "       [[  6,  15]],\n",
      "\n",
      "       [[  6, 232]],\n",
      "\n",
      "       [[234, 232]],\n",
      "\n",
      "       [[235, 233]],\n",
      "\n",
      "       [[238, 233]],\n",
      "\n",
      "       [[239, 232]],\n",
      "\n",
      "       [[244, 232]],\n",
      "\n",
      "       [[244,  15]],\n",
      "\n",
      "       [[242,  15]],\n",
      "\n",
      "       [[241,  14]]])]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "image = cv2.imread('images/313.png')\n",
    "cv2.imshow('Input Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Find Canny edges\n",
    "edged = cv2.Canny(gray, 30, 200)\n",
    "cv2.imshow('Canny Edges', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#finding Contours\n",
    "# Use a copy your image, eg. edge.copy(), since findContours alters the image\n",
    "contours, hierarchy = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cv2.imshow('Canny Edges After Contouring', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(\"Numper of Contours found = \" + str(len(contours)))\n",
    "print(contours)\n",
    "# Drow all contours \n",
    "# Use '-1' as the 3rd parameter to draw all\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/314.png\" style=\"display: block; margin: 10px auto; max-width:97%;\">\n",
    "<img src=\"images/315.png\" style=\"display: block; margin: 10px auto; max-width:97%;\">\n",
    "<img src=\"images/316.png\" style=\"display: block; margin: 10px auto; max-width:97%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numper of Contours found = 4\n",
      "[array([[[273, 115]],\n",
      "\n",
      "       [[273, 224]],\n",
      "\n",
      "       [[367, 224]],\n",
      "\n",
      "       [[368, 223]],\n",
      "\n",
      "       [[368, 115]]]), array([[[272,  71]],\n",
      "\n",
      "       [[272,  73]],\n",
      "\n",
      "       [[273,  74]],\n",
      "\n",
      "       [[273,  76]],\n",
      "\n",
      "       [[272,  77]],\n",
      "\n",
      "       [[266,  77]],\n",
      "\n",
      "       [[266,  79]],\n",
      "\n",
      "       [[272,  79]],\n",
      "\n",
      "       [[273,  80]],\n",
      "\n",
      "       [[273,  82]],\n",
      "\n",
      "       [[272,  83]],\n",
      "\n",
      "       [[272,  85]],\n",
      "\n",
      "       [[278,  85]],\n",
      "\n",
      "       [[276,  85]],\n",
      "\n",
      "       [[275,  84]],\n",
      "\n",
      "       [[274,  85]],\n",
      "\n",
      "       [[273,  85]],\n",
      "\n",
      "       [[272,  84]],\n",
      "\n",
      "       [[273,  83]],\n",
      "\n",
      "       [[273,  73]],\n",
      "\n",
      "       [[272,  72]],\n",
      "\n",
      "       [[273,  71]],\n",
      "\n",
      "       [[274,  71]],\n",
      "\n",
      "       [[275,  72]],\n",
      "\n",
      "       [[276,  71]],\n",
      "\n",
      "       [[278,  71]],\n",
      "\n",
      "       [[279,  72]],\n",
      "\n",
      "       [[278,  73]],\n",
      "\n",
      "       [[277,  73]],\n",
      "\n",
      "       [[276,  74]],\n",
      "\n",
      "       [[276,  82]],\n",
      "\n",
      "       [[277,  83]],\n",
      "\n",
      "       [[279,  83]],\n",
      "\n",
      "       [[277,  83]],\n",
      "\n",
      "       [[276,  82]],\n",
      "\n",
      "       [[276,  80]],\n",
      "\n",
      "       [[277,  79]],\n",
      "\n",
      "       [[281,  79]],\n",
      "\n",
      "       [[281,  77]],\n",
      "\n",
      "       [[277,  77]],\n",
      "\n",
      "       [[276,  76]],\n",
      "\n",
      "       [[276,  74]],\n",
      "\n",
      "       [[277,  73]],\n",
      "\n",
      "       [[279,  73]],\n",
      "\n",
      "       [[279,  71]]]), array([[[387,  44]],\n",
      "\n",
      "       [[387, 229]],\n",
      "\n",
      "       [[388, 230]],\n",
      "\n",
      "       [[388, 231]],\n",
      "\n",
      "       [[586, 231]],\n",
      "\n",
      "       [[586,  44]]]), array([[[ 10,  10]],\n",
      "\n",
      "       [[  9,  11]],\n",
      "\n",
      "       [[  8,  11]],\n",
      "\n",
      "       [[  8, 228]],\n",
      "\n",
      "       [[  9, 228]],\n",
      "\n",
      "       [[ 10, 229]],\n",
      "\n",
      "       [[243, 229]],\n",
      "\n",
      "       [[244, 228]],\n",
      "\n",
      "       [[246, 228]],\n",
      "\n",
      "       [[246,  11]],\n",
      "\n",
      "       [[ 24,  11]],\n",
      "\n",
      "       [[ 23,  10]]])]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "image = cv2.imread('images/317.png')\n",
    "cv2.imshow('Input Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Find Canny edges\n",
    "edged = cv2.Canny(gray, 30, 200)\n",
    "cv2.imshow('Canny Edges', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#finding Contours\n",
    "# Use a copy your image, eg. edge.copy(), since findContours alters the image\n",
    "contours, hierarchy = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cv2.imshow('Canny Edges After Contouring', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(\"Numper of Contours found = \" + str(len(contours)))\n",
    "print(contours)\n",
    "# Drow all contours \n",
    "# Use '-1' as the 3rd parameter to draw all\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numper of Contours found = 11\n",
      "[array([[[273, 116]],\n",
      "\n",
      "       [[274, 115]],\n",
      "\n",
      "       [[367, 115]],\n",
      "\n",
      "       [[368, 116]],\n",
      "\n",
      "       [[368, 222]],\n",
      "\n",
      "       [[366, 224]],\n",
      "\n",
      "       [[274, 224]],\n",
      "\n",
      "       [[273, 223]]]), array([[[273, 115]],\n",
      "\n",
      "       [[273, 224]],\n",
      "\n",
      "       [[367, 224]],\n",
      "\n",
      "       [[368, 223]],\n",
      "\n",
      "       [[368, 115]]]), array([[[ 71,  86]],\n",
      "\n",
      "       [[ 72,  85]],\n",
      "\n",
      "       [[197,  85]],\n",
      "\n",
      "       [[199,  87]],\n",
      "\n",
      "       [[199, 166]],\n",
      "\n",
      "       [[197, 168]],\n",
      "\n",
      "       [[196, 168]],\n",
      "\n",
      "       [[195, 167]],\n",
      "\n",
      "       [[ 72, 167]],\n",
      "\n",
      "       [[ 71, 166]]]), array([[[ 71,  85]],\n",
      "\n",
      "       [[ 71, 167]],\n",
      "\n",
      "       [[195, 167]],\n",
      "\n",
      "       [[196, 168]],\n",
      "\n",
      "       [[197, 168]],\n",
      "\n",
      "       [[198, 167]],\n",
      "\n",
      "       [[199, 167]],\n",
      "\n",
      "       [[199,  87]],\n",
      "\n",
      "       [[197,  85]]]), array([[[276,  78]],\n",
      "\n",
      "       [[277,  77]],\n",
      "\n",
      "       [[280,  77]],\n",
      "\n",
      "       [[281,  78]],\n",
      "\n",
      "       [[280,  79]],\n",
      "\n",
      "       [[277,  79]]]), array([[[266,  78]],\n",
      "\n",
      "       [[267,  77]],\n",
      "\n",
      "       [[272,  77]],\n",
      "\n",
      "       [[273,  78]],\n",
      "\n",
      "       [[272,  79]],\n",
      "\n",
      "       [[267,  79]]]), array([[[272,  71]],\n",
      "\n",
      "       [[272,  73]],\n",
      "\n",
      "       [[273,  74]],\n",
      "\n",
      "       [[273,  76]],\n",
      "\n",
      "       [[272,  77]],\n",
      "\n",
      "       [[266,  77]],\n",
      "\n",
      "       [[266,  79]],\n",
      "\n",
      "       [[272,  79]],\n",
      "\n",
      "       [[273,  80]],\n",
      "\n",
      "       [[273,  82]],\n",
      "\n",
      "       [[272,  83]],\n",
      "\n",
      "       [[272,  85]],\n",
      "\n",
      "       [[278,  85]],\n",
      "\n",
      "       [[276,  85]],\n",
      "\n",
      "       [[275,  84]],\n",
      "\n",
      "       [[274,  85]],\n",
      "\n",
      "       [[273,  85]],\n",
      "\n",
      "       [[272,  84]],\n",
      "\n",
      "       [[273,  83]],\n",
      "\n",
      "       [[273,  73]],\n",
      "\n",
      "       [[272,  72]],\n",
      "\n",
      "       [[273,  71]],\n",
      "\n",
      "       [[274,  71]],\n",
      "\n",
      "       [[275,  72]],\n",
      "\n",
      "       [[276,  71]],\n",
      "\n",
      "       [[278,  71]],\n",
      "\n",
      "       [[279,  72]],\n",
      "\n",
      "       [[278,  73]],\n",
      "\n",
      "       [[277,  73]],\n",
      "\n",
      "       [[276,  74]],\n",
      "\n",
      "       [[276,  82]],\n",
      "\n",
      "       [[277,  83]],\n",
      "\n",
      "       [[279,  83]],\n",
      "\n",
      "       [[277,  83]],\n",
      "\n",
      "       [[276,  82]],\n",
      "\n",
      "       [[276,  80]],\n",
      "\n",
      "       [[277,  79]],\n",
      "\n",
      "       [[281,  79]],\n",
      "\n",
      "       [[281,  77]],\n",
      "\n",
      "       [[277,  77]],\n",
      "\n",
      "       [[276,  76]],\n",
      "\n",
      "       [[276,  74]],\n",
      "\n",
      "       [[277,  73]],\n",
      "\n",
      "       [[279,  73]],\n",
      "\n",
      "       [[279,  71]]]), array([[[387,  45]],\n",
      "\n",
      "       [[388,  44]],\n",
      "\n",
      "       [[585,  44]],\n",
      "\n",
      "       [[586,  45]],\n",
      "\n",
      "       [[586, 230]],\n",
      "\n",
      "       [[585, 231]],\n",
      "\n",
      "       [[389, 231]],\n",
      "\n",
      "       [[387, 229]]]), array([[[387,  44]],\n",
      "\n",
      "       [[387, 229]],\n",
      "\n",
      "       [[388, 230]],\n",
      "\n",
      "       [[388, 231]],\n",
      "\n",
      "       [[586, 231]],\n",
      "\n",
      "       [[586,  44]]]), array([[[ 10,  10]],\n",
      "\n",
      "       [[ 23,  10]],\n",
      "\n",
      "       [[ 24,  11]],\n",
      "\n",
      "       [[245,  11]],\n",
      "\n",
      "       [[246,  12]],\n",
      "\n",
      "       [[246, 227]],\n",
      "\n",
      "       [[245, 228]],\n",
      "\n",
      "       [[244, 228]],\n",
      "\n",
      "       [[243, 229]],\n",
      "\n",
      "       [[ 10, 229]],\n",
      "\n",
      "       [[  8, 227]],\n",
      "\n",
      "       [[  8,  12]]]), array([[[ 10,  10]],\n",
      "\n",
      "       [[  9,  11]],\n",
      "\n",
      "       [[  8,  11]],\n",
      "\n",
      "       [[  8, 228]],\n",
      "\n",
      "       [[  9, 228]],\n",
      "\n",
      "       [[ 10, 229]],\n",
      "\n",
      "       [[243, 229]],\n",
      "\n",
      "       [[244, 228]],\n",
      "\n",
      "       [[246, 228]],\n",
      "\n",
      "       [[246,  11]],\n",
      "\n",
      "       [[ 24,  11]],\n",
      "\n",
      "       [[ 23,  10]]])]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "image = cv2.imread('images/317.png')\n",
    "cv2.imshow('Input Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Find Canny edges\n",
    "edged = cv2.Canny(gray, 30, 200)\n",
    "cv2.imshow('Canny Edges', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#finding Contours\n",
    "# Use a copy your image, eg. edge.copy(), since findContours alters the image\n",
    "contours, hierarchy = cv2.findContours(edged, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cv2.imshow('Canny Edges After Contouring', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "print(\"Numper of Contours found = \" + str(len(contours)))\n",
    "print(contours)\n",
    "# Drow all contours \n",
    "# Use '-1' as the 3rd parameter to draw all\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>032 Sorting Contours</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Numper of contours found = ', 6)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "image = cv2.imread('images/320.png')\n",
    "cv2.imshow('Input Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Create a black image with same dimenstions as our loaded image\n",
    "blank_image = np.zeros((image.shape[0], image.shape[1], 3))\n",
    "\n",
    "#Create a copy of our original image\n",
    "original_image = image\n",
    "\n",
    "#Grayscale our image\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Find Canny image\n",
    "edged =  cv2.Canny(image, 50, 200)\n",
    "cv2.imshow('1- Canny Edges', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Find contours and print how many were found\n",
    "contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "print('Numper of contours found = ', len(contours))\n",
    "\n",
    "#Draw all contours\n",
    "cv2.drawContours(blank_image, contours, -1, (0, 255, 0), 3)\n",
    "cv2.imshow('2- All Contours over blank image', blank_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Draw all contours over blank image\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 3)\n",
    "cv2.imshow('3- All Contours', image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's now sort contours by area size</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contor Areas before sorting [11663.0, 13060.5, 37824.0, 1.0, 8.5, 51254.5]\n",
      "Contor Areas after sorting [51254.5, 37824.0, 13060.5, 11663.0, 8.5, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "image = cv2.imread('images/320.png')\n",
    "orginal_image = image \n",
    "\n",
    "# Function we'll use to display contour area\n",
    "def get_contour_areas(contours):\n",
    "    # returns the areas of all contours as list\n",
    "    all_areas = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        all_areas.append(area)\n",
    "    return all_areas\n",
    "\n",
    "#Grayscale our image\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Find Canny image\n",
    "edged =  cv2.Canny(image, 50, 200)\n",
    "\n",
    "# Find contours and print how many were found\n",
    "contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# Print contours area before sorting \n",
    "print \"Contor Areas before sorting\",\n",
    "print get_contour_areas(contours)\n",
    "\n",
    "#sort contours large to small\n",
    "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "#sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:3]\n",
    "\n",
    "print \"Contor Areas after sorting\",\n",
    "print get_contour_areas(sorted_contours)\n",
    "\n",
    "# Iterate over our contours and draw one at a time\n",
    "for c in sorted_contours:\n",
    "    cv2.drawContours(orginal_image, [c], -1, (0, 255,0), 3)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.imshow('Contours by area', orginal_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "image = cv2.imread('images/320.png')\n",
    "orginal_image = image \n",
    "\n",
    "# Function we'll use for sorting by position\n",
    "def x_cord_contour(contours):\n",
    "    #Returns the X cordinate for the contour centroid\n",
    "    if cv2.contourArea(contours)>10:\n",
    "        M = cv2.moments(contours)\n",
    "        return(int(M['m10']/m['m00']))\n",
    "\n",
    "def label_contour_center(image, c,1):\n",
    "    #Places a red circle on the centers of contours\n",
    "    M = cv2.moments(c)\n",
    "    cx= int(M['m10']/M['m00'])\n",
    "    cy= int(M['m10']/M['m00'])\n",
    "    \n",
    "    #Draw the contour number on the image\n",
    "    cv2.circle(image, (cx, cy), 10, (0,0,255), -1)\n",
    "    return image\n",
    "\n",
    "# computer Center of Mass or centroid and draw them on out image\n",
    "for (i, c) in enumerate(contours):\n",
    "    orig = label_contour_center(image, c, i)\n",
    "\n",
    "cv.imshow('4- contour Center', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Sort by left to right using our x_cord_contour function\n",
    "contours_left_to_right = sorted(contours, key = x_cord_contour, reverse = False)\n",
    "\n",
    "#labling contours left to right \n",
    "for(i,c) in enumerate(contours_left_to_right):\n",
    "    cv2.drawContours(orgina_image, [c], -1, (0,0,255), 3)\n",
    "    M = cv2.moments(c)\n",
    "    cx= int(M['m10']/M['m00'])\n",
    "    cy= int(M['m10']/M['m00'])\n",
    "    cv2.putText(orginal_image, str(i+1), (cx,cy), cv2.FONT_HERSHEY_SIMLEX, 1,(0, 255, 0), 2)\n",
    "    cv.imshow('6- left to right contour ', orginal_image)\n",
    "    cv2.waitKey(0)\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    \n",
    "    #Let's now crop each contour and save these images\n",
    "    cropped_contour = orginal_image[y:y+h, x:x+w]\n",
    "    image_name = \"output_shape_number_\" + srt(i+1) + \".jpg\"\n",
    "    print image_name\n",
    "    cv2.imwrite(image_name, cropped_contour)\n",
    "    \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
